<!DOCTYPE html>
<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Anderson Avila</title>
  <meta name="description" content="">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://localhost:4000/deep/prob1">
  <link rel="alternate" type="application/rss+xml" title="Anderson Avila" href="http://localhost:4000/feed.xml">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Anderson Avila</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
        
          
        
          
          <a class="page-link" href="/cv/cv">Curriculum</a>
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title"></h1>
  </header>

  <div class="post-content">
    <header class="post-header">
    <h1 class="post-title">Probability</h1>
  </header>
<p><br /><br /></p>
<p>
<i>Probability theory is nothing but common sense reduced to calculation</i> - Pierre Simon Laplace
<br /><br />

The word probability is commonly employed in informal conversations. It is used to express the chance that something will happen. Here are some classical examples: "I will probably work on the weekend" or "It is unlikely that she will comeback." 

<br /><br />

<!-- Machine Learning A Probabilistic Approach - page 27 -->
There are at least two formal interpretation of probability, namely <b>frequentist</b> and <b>Bayesian</b>. The first express the number or frequency of an outcome event on a long run experiment. For example, if a coin is tossed 10.000 times how many times it will land heads? If it is a fair coin, we expect to see heads roughly half of the time. Another good example is the experiment of drawing a hand of cards. If we perform the experiment many times, we will find that the probability <i>p</i> of drawing certain card actually equals the proportion of times that particular card was drawn in a long run.

<br /><br />

The second interpretation is used to express uncertainty or likelihood of an event without necessary performing an experiment many times. This is the Bayesian probability. It uses information related to some outcome in order to calculate the actual probability (also referred to as degrees of belief) of an event. For example, if we want to know the probability of a person having a centain disease we could use Bayes' theorem to calculate this probability using not just the probability of any person having the disease, but combining it with relevant information such as symptoms, age, laboratorial tests, etc. The frequentist approach is not feasable in this case since it would require many patients with the exactly same condition so that we could find the proportion (also probability) of that disease.

<br /><br />

<!-- Deep Learning book - page 56 -->
On the contrary of many branches of Computer Science that deals with deterministic events, for Machine Learning the second interpretation of probability (Bayesian interpretation) is extremely relevant. That's because Machine Learning algorithms are required to have the ability to reason in the presence of uncertainty. For example, we might want our algorithm to predict customer behavior online and infer which products he/she would be mostly interested to add in the cart. Of couse, we can create a deterministic model for that, but it can be quite complicated to do so. Image the following rule "Women at age x that buy product A and B are likely to buy product C, except if they do not have children". It could be very hard to develop whereas a model that simply states that most women buy product C based on some probability distribution would be less expensive to develop.

<br /><br />

<b>Discrete Random Variables</b>

<br /><br />

A discrete random variable is a variable that can assume different values randomly. Image the outcome of tossing a coin or a die. We could represent the set of outcomes for these two events using the discrete random variables <i>A</i> and <i>B</i>, where <i>A</i> = {tail, head} and <i>B</i> = {1, 2, 3, 4, 5, 6}. The probability of <i>A</i> is (0 &lt;= P(A) &lt;= 1). Since it is a random process, a probability distribution (or probability mass function) is associated with each discrete variable and is used to describe the probability of each event. For example, the probability of <i>P</i>(<i>A</i>=tail) is 0.5 and the probability of <i>P</i>(<i>B</i>=1) is 0.16, and it is the same for each event or state of these discrete random variables. Both are uniform distributions in this case, i.e. every event has the same probability as described in the figure bellow. Generally, we denote the probability of an event or state <i>A = a</i> as <i>P(A=a)</i>. 

<br /><br />

<center>
<img src="/img/coin.png" height="242" width="300" />
<img src="/img/die.png" height="242" width="300" />
</center>

<!--
Code for generating graph in R

# Tossing a coin
n <- floor(runif(1000000)*2)
t <- table(n)
barplot(t/1000000, xlab="A = 'Tossing a coin'", ylab="P(A)", names.arg=c("Tail", "Head"), col="darkred")

# Tossing a die
n <- floor(runif(1000000)*6) + 1
t <- table(n)
barplot(t/1000000, xlab="B = 'Tossing a die'", ylab="P(B)", col="darkred")

-->
 
<br /><br />

<b>Union of two events</b>

<br /><br />
The union of two events A and B describes the probability of occurence the one or the other and can be expressed as follows:
<br /><br />

<i>P</i>(<i>A U B</i>) = <i>P</i>(<i>A</i>) + <i>P</i>(<i>B</i>) - <i>P</i>(<i>A &#x2229; B</i>)

<br /><br />

For the two experiments aforementioned, the events are independent and for that reason <i>P</i>(<i>A &#x2229; B</i>) = 0. Notice the first figure bellow where there is no overlap or intersection between the set of two events. It well represents the events described above. The probability of the union of tossing a coin and a die is just the sum of the two probabilities. The second figure shows the case where the events are dependent and is possible to observe some overlap between the two. In this case, <i>P</i>(<i>A &#x2229; B</i>) &lt;&gt; 0 and we must subtract it from the sum of individual probabilities.

<br /><br />

<center>
<img src="/img/independent.png" height="202" width="600" />
</center>

<br /><br />

<b>Joint probabilities</b>

<br /><br />

The joint probability of two dependent events is the probability of both occuring, i.e. the joint probability of event A and B is given by

<br /><br />

<i>P</i>(<i>A &#x2229; B</i>) = <i>P</i>(<i>A | B</i>)<i>P</i>(<i>B</i>)

<br /><br />

This is also called the <b>product rule</b> and, of course, we consider here the events to be dependents of each other because the fact that one event happened may influence the probability of the other event as described in the equation above. We can also think of the individual states of B and define the marginal probability using the <b>sum rule</b> as shown below

<br /><br />

<i>P</i>(<i>A</i>) = &Sigma;<i>P</i>(<i>A &#x2229; B</i>) = &Sigma;<i>P</i>(<i>A | B = b</i>)<i>P</i>(<i>B = b</i>)

<br /><br />

And by symmetry

<br /><br />

<i>P</i>(<i>B</i>) = &Sigma;<i>P</i>(<i>B &#x2229; A</i>) = &Sigma;<i>P</i>(<i>B | A = a</i>)<i>P</i>(<i>A = a</i>)

<br /><br />

<b>Conditional Probability</b>

<br /><br />

Conditional probability is the probability of some event given that another event is true. For example, we could want to know the probability of having a disease given that the patient tested positive. Conditional probability can be defined as

<br /><br />

<i>P</i>(<i>A | B</i>) = <i>P</i>(<i>B &#x2229; A</i>) / <i>P</i>(<i>B</i>)

<br /><br />

<b>Bayes rule</b>

<br /><br />

Bayes rule results of the combination of conditional probability and sum and product rules. The idea behind this rule is that if we know <i>P</i>(<i>A | B</i>), then the probability of <i>P</i>(<i>B | A</i>) is given by

<br /><br />

<i>P</i>(<i>B | A</i>) = <i>P</i>(<i>B | A</i>)<i>P</i>(<i>B</i>) / <i>P</i>(<i>B</i>)
<br /><br />

We can actually derivate it from conditional probability. Let's consider

<br /><br />

<i>P</i>(<i>A &#x2229; B</i>) = <i>P</i>(<i>A | B</i>)<i>P</i>(<i>B</i>)

<br /><br />

Which leads to

<br /><br />

<i>P</i>(<i>A | B</i>) = <i>P</i>(<i>A &#x2229; B</i>) / <i>P</i>(<i>B</i>)

<br /><br />

Since

<br /><br />

<i>P</i>(<i>A &#x2229; B</i>) = <i>P</i>(<i>B &#x2229; A</i>) = <i>P</i>(<i>B | A</i>)<i>P</i>(<i>A</i>)

<br /><br />

Then

<br /><br />

<i>P</i>(<i>A | B</i>) = <i>P</i>(<i>B | A</i>)<i>P</i>(<i>A</i>) / <i>P</i>(<i>B</i>)

<br /><br />

The idea here is that we can use evidence of B to update our belief about hypothesis A. In the formula above, the so-called posterior belief <i>P</i>(<i>A | B</i>) is obtained by using prior knowledge about <i>P</i>(<i>A</i>) and multiplying it by <i>P</i>(<i>B | A</i>), which express the likelihood of B occurring considering that A is true.

<br /><br />

As an example, let's think of the following situation:

<br /><br />

Let A represent the event "Person is an engineer"

<br /><br />

Let B represent the event "Person likes math"

<br /><br />

And we know that <i>P</i>(<i>A</i>) = 0.2, i.e. our prior knowledge is that the probability of a person being an engineer is 20%. This probability can be updated if we have information about event B (of course the two events can not be independent in this case). So, we want to compute <i>P</i>(<i>A | B</i>) or the probability of being an engineer given that the person likes math. And this is going to be, according to Bayes rule, as follows

<br /><br />

<i>P</i>(<i>A | B</i>) = <i>P</i>(<i>B | A</i>)<i>P</i>(<i>A</i>) / <i>P</i>(<i>B</i>) or

<br /><br />

<i>P</i>(<i>A | B</i>) = <i>P</i>(<i>B | A</i>)<i>P</i>(<i>A</i>) / (<i>P</i>(<i>B | A</i>)<i>P</i>(<i>A</i>) + <i>P</i>(<i>B' | A</i>)<i>P</i>(<i>A</i>)), since <i>P</i>(<i>B</i>)  = <i>P</i>(<i>B | A</i>)<i>P</i>(<i>A</i>) + <i>P</i>(<i>B' | A</i>)<i>P</i>(<i>A</i>)

<br /><br />

It states that the probability of being an engineer given that the person likes math is equal the probability of liking math and being a engineer divided by the probability of not liking math and being an engineer plus the probability of liking math and not being an engineer.

</p>


  </div>

</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Anderson Avila</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>Anderson Avila</li>
          <li><a href="mailto:anderson.avila@gmail.com">anderson.avila@gmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/aravila"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">aravila</span></a>

          </li>
          

          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
