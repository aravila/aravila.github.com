---
layout: page
title: 
permalink: /deep/activ
---

  <header class="post-header">
    <h1 class="post-title">Activation Functions</h1>
  </header>
  <br><br>
<p>

Activation Function is a function applied to the weighted sum of the inputs as depicted in the image bellow. It is a continuos, monotonacally increasing, and differentiable function. They are used in the hidden layers as well as in the output layer. The activation function used in the output unit defines which loss function (mean square error or cross entropy) to be used.

<center><img src="{{ site.baseurl }}/img/blog/activation.png" height="200" width="350"></center>
<br><br>

The most simple unit is the one based on a linear transformation (or affine transformation).
No nonlinearity is encountered in such case: <i><b>y = W'*h + b</b></i>.


</p>
